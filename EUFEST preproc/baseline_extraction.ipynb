{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edaeb2d-ee22-4126-b801-022409e8a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 5)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Folder name with original input data.\n",
    "dir_input = 'Raw data/'\n",
    "\n",
    "#Folder name for output data.\n",
    "dir_output = 'Baseline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db871a8-42e8-47f7-8adf-8299c429996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#Data Extraction\n",
    "################################################################################\n",
    "# Sergio Mena Ortega, 2023\n",
    "\n",
    "\n",
    "#------------- Camberwell Assessment of Needs (CAN) -----------------------------\n",
    "# OUTPUT dataframe: df_cam\n",
    "\n",
    "df_cam = pd.read_excel(dir_input+'camber_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "\n",
    "#Replacing \"9\" with NaN - > unknown.\n",
    "df_cam.replace(9, np.nan, inplace=True)\n",
    "\n",
    "#Remove non-applicable columns.\n",
    "df_cam = df_cam.drop(columns = ['V1CA021', 'V1CA163'])\n",
    "\n",
    "#Calculate sum of no/low need, moderate need and high need from relevant columns. \n",
    "df_cam_var_labels = pd.read_excel(dir_input+'camber_070727.xlsx', sheet_name = 'Variable labels', header = None)\n",
    "df_cam_var_labels = df_cam_var_labels[df_cam_var_labels[0].str.startswith('V1')]\n",
    "filtered_codes = df_cam_var_labels[df_cam_var_labels[1] == \"{0.0: 'No', 1.0: 'No/Moderate', 2.0: 'Unmet need', 9.0: 'not known'}\"][0].values\n",
    "#Add SUM columns\n",
    "df_cam['V1CASUM_NO'] = (df_cam[filtered_codes] == 0).sum(axis=1)\n",
    "df_cam['V1CASUM_MODERATE'] = (df_cam[filtered_codes] == 1).sum(axis=1)\n",
    "df_cam['V1CASUM_UNMET'] = (df_cam[filtered_codes] == 2).sum(axis=1)\n",
    "# Set sums to NaN if all values in the corresponding rows are NaN.\n",
    "nan_rows = df_cam[filtered_codes].isna().all(axis=1)\n",
    "df_cam.loc[nan_rows, ['V1CASUM_NO', 'V1CASUM_MODERATE', 'V1CASUM_UNMET']] = np.nan\n",
    "\n",
    "\n",
    "#------------- Calgary Depression scale for Schizophrenia ----------------\n",
    "# OUTPUT dataframe: df_cdss\n",
    "\n",
    "#Replacing \"9\" with NaN - > unknown.\n",
    "df_cdss = pd.read_excel(dir_input+'cdss_070904.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)').replace(9, np.nan)\n",
    "\n",
    "#Add a total score variable.\n",
    "df_cdss['V1CDTOTAL'] = df_cdss[['V1CD01', 'V1CD02', 'V1CD03', 'V1CD04', 'V1CD05', 'V1CD06', 'V1CD07', 'V1CD08', 'V1CD09']].sum(axis = 1)\n",
    "\n",
    "#Assign nan when all values are nan. \n",
    "df_cdss.loc[df_cdss[['V1CD01', 'V1CD02', 'V1CD03', 'V1CD04', 'V1CD05', 'V1CD06', 'V1CD07', 'V1CD08', 'V1CD09']].isna().all(axis = 1), 'V1CDTOTAL'] = np.nan\n",
    "\n",
    "\n",
    "#------------- Clinical global impression.--------------------------------\n",
    "# OUTPUT dataframe: df_cgi\n",
    "# - Replacement of 0's (not assessed) with NaN.\n",
    "df_cgi = pd.read_excel(dir_input+'cgi_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)').replace(0, np.nan)\n",
    "\n",
    "\n",
    "#------------- Concominant medication -------------------------------------------\n",
    "# OUTPUT dataframe: df_concommed\n",
    "#Extract type of drug based on ATC Code (https://www.whocc.no/atc_ddd_index/?code=N&showdescription=no)\n",
    "def categorize_drug(atcode):\n",
    "    if type(atcode)  == str: \n",
    "        if atcode[:3] == 'N01':\n",
    "            return 'CONCOMMED_ANESTHETICS'\n",
    "        elif atcode[:3] == 'N02':\n",
    "            return 'CONCOMMED_ANALGESICS'\n",
    "        elif atcode[:3] == 'N03':\n",
    "            return 'CONCOMMED_ANTIEPILEPTICS'\n",
    "        elif atcode[:3] == 'N04':\n",
    "            return 'CONCOMMED_ANTI-PARKINSON DRUGS'\n",
    "        elif atcode[:3] == 'N05':\n",
    "            return 'CONCOMMED_PSYCHOLEPTICS'\n",
    "        elif atcode[:3] == 'N06':\n",
    "            return 'CONCOMMED_PSYCHOANALEPTICS'\n",
    "        elif atcode[:3] == 'N07':\n",
    "            return 'CONCOMMED_OTHER_NSD'\n",
    "        else:\n",
    "            return 'CONCOMMED_OTHER_NON-NSD'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_concommed = pd.read_excel(dir_input+'Concommed_070727.xlsx', sheet_name = 'Data', usecols = ['crfnr','dStart1', 'dEind1', 'atcode1'])\n",
    "#Extract dates of baseline visit. \n",
    "df_v1_date = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(rr001|crfnr)')\n",
    "#Left merge. \n",
    "df_concommed  = pd.merge(df_concommed, df_v1_date, on='crfnr', how='left')\n",
    "# Create \"concomminant before\" variable, when date of V1 is after start of first concominant prescription.\n",
    "df_concommed['CONCOMMED_BEFORE'] = (df_concommed['rr001_dat'] >= df_concommed['dStart1']).astype(int) \n",
    "# Set CONCOMMED_BEFORE to NaN where R1_pi02_dat is NaN or NaT.\n",
    "df_concommed.loc[df_concommed['dStart1'].isna(), 'CONCOMMED_BEFORE'] = np.nan\n",
    "# Store patient ID and CONCOMMED_BEFORE.\n",
    "df_concommed = df_concommed[['crfnr','CONCOMMED_BEFORE']]\n",
    "\n",
    "# Define the list of drug prescriptions (it goes up to 25)\n",
    "medications = [str(i) for i in range(1, 25)]\n",
    "\n",
    "list_of_classes = ['CONCOMMED_ANESTHETICS', 'CONCOMMED_ANALGESICS', 'CONCOMMED_ANTIEPILEPTICS', \n",
    "                   'CONCOMMED_ANTI-PARKINSON DRUGS', 'CONCOMMED_PSYCHOLEPTICS', \n",
    "                   'CONCOMMED_PSYCHOANALEPTICS', 'CONCOMMED_OTHER_NSD', 'CONCOMMED_OTHER_NON-NSD']\n",
    "\n",
    "list_of_TYPE = []\n",
    "\n",
    "for medication in medications:\n",
    "    # Read intervention data\n",
    "    cols = ['crfnr','dStart'+medication, 'dEind'+medication, 'atcode'+medication]\n",
    "    df_concommed_each = pd.read_excel(dir_input+'Concommed_070727.xlsx', sheet_name='Data', usecols = cols)\n",
    "\n",
    "    # Left merge\n",
    "    df_concommed_each = pd.merge(df_concommed_each, df_v1_date, on='crfnr', how='left')\n",
    "\n",
    "    # Create \"concommitant medication before\" variable.\n",
    "    df_concommed_each['CONCOMMED_BEFORE'] = (df_concommed_each['rr001_dat'] >= df_concommed_each['dStart'+medication]).astype(int)\n",
    "    \n",
    "    # Mapping of codes to type of drug. \n",
    "    for class_drug in list_of_classes:   \n",
    "        df_concommed_each[class_drug] = df_concommed_each['atcode'+medication].apply(lambda x: 1 if categorize_drug(x) == class_drug else 0) \n",
    "\n",
    "    \n",
    "    # Set dummy codes for types of drugs to 0 if CONCOMMED_BEFORE is 0.\n",
    "    df_concommed_each.loc[df_concommed_each['CONCOMMED_BEFORE'] == 0, list_of_classes] = 0\n",
    "    # Append it to the list. \n",
    "    list_of_TYPE.append(df_concommed_each[list_of_classes])\n",
    "\n",
    "    \n",
    "#Record type of concommitant medications and overlap them. If one type is prescribed more than once, it is stil set to 1. \n",
    "df_concommed.loc[:, list_of_classes] = np.where(np.sum(list_of_TYPE, axis=0)>= 1, 1, 0)\n",
    "\n",
    "# If INTERV_BEFORE is n.nan, then all interventions are nan (since it is not clear if v1 was before or after).\n",
    "df_concommed.loc[df_concommed['CONCOMMED_BEFORE'].isna(), list_of_classes] = np.nan\n",
    "\n",
    "#IMPORTANT NEW ADDITION: If there is no record of concomitant medication, then set CONCOMMED_BEFORE to 0 as well as the classes variables.\n",
    "df_v1 = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(crfnr)')\n",
    "# Get entries of patients not in df_concommed and set all variables to 0.\n",
    "df_no_concommed = df_v1[~df_v1['crfnr'].isin(df_concommed['crfnr'])].copy()\n",
    "df_no_concommed[['CONCOMMED_BEFORE', list_of_classes]] = 0\n",
    "# Concatenate entries in the register and not in the register.\n",
    "df_concommed = pd.concat([df_concommed, df_no_concommed]).sort_values(by='crfnr')\n",
    "\n",
    "## NOTE that NaN entries in this dataset involve patients where there is a register of concomitant medication, but the dates are \n",
    "#not clear and therefore not known whether it was before or after baseline.\n",
    "\n",
    "\n",
    "#------------- Edinburgh Handness Inventory -------------------------------------\n",
    "# OUTPUT dataframe: df_ehi\n",
    "df_ehi = pd.read_excel(dir_input+'ehi_070727.xlsx').filter(regex='^(vr|f|crfnr)').filter(regex='^(?!ftl|ftr|ftu|vr1tl|vr1tr).*$')\n",
    "#Get totals of right and left hand scores.\n",
    "vr_l_cols = [col for col in df_ehi.columns if col.startswith('vr') and col.endswith('l')]\n",
    "vr_r_cols = [col for col in df_ehi.columns if col.startswith('vr') and col.endswith('r')]\n",
    "\n",
    "# Check for NaN values - > NaN sum, and otherwise calculate weighted sums accordingly\n",
    "df_ehi['w_sum_vr_l'] = df_ehi[vr_l_cols].apply(lambda row: row.sum() / row.count() if not row.isna().all() else np.nan, axis=1)\n",
    "df_ehi['w_sum_vr_r'] = df_ehi[vr_r_cols].apply(lambda row: row.sum() / row.count() if not row.isna().all() else np.nan, axis=1)\n",
    "\n",
    "# Calculate Laterality Quotient Score with consideration for NaN values, based on: https://link.springer.com/referenceworkentry/10.1007/978-0-387-79948-3_68\n",
    "df_ehi['LQS_SCORE'] = np.where((df_ehi['w_sum_vr_r'].isnull()) | (df_ehi['w_sum_vr_l'].isnull()),\n",
    "                               np.nan,\n",
    "                               100 * (df_ehi['w_sum_vr_r'] - df_ehi['w_sum_vr_l']) / (df_ehi['w_sum_vr_r'] + df_ehi['w_sum_vr_l']))\n",
    "\n",
    "#Dummy coding f1-9 variables. \n",
    "# Assuming df is your DataFrame and f1-f9 are the columns to be dummy coded\n",
    "f_cols =  [col for col in df_ehi.columns if col.startswith('f')]\n",
    "\n",
    "# Map the values into categorical, otherwise it does not detect columns.\n",
    "df_ehi[f_cols] = df_ehi[f_cols].replace({0: 'unknown', 1: 'left', 2: 'right'})\n",
    "\n",
    "# Dummy code the variables using pd.get_dummies individually in a for loop.\n",
    "for col in f_cols:\n",
    "    df_dummies = pd.get_dummies(df_ehi[col], prefix=col)\n",
    "    #If the rows are all 0's (meaning the original var was NaN) set the dummy variables to NaN.\n",
    "    mask_all_zeros = (df_dummies.sum(axis=1) == 0)\n",
    "    df_dummies[df_dummies.sum(axis=1) == 0] = np.nan\n",
    "    #Add to dataframe and remove original.\n",
    "    df_ehi = pd.concat([df_ehi, df_dummies], axis=1)\n",
    "    df_ehi.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Check for NaN values - > NaN sum, and otherwise calculate weighted sums accordingly\n",
    "left_cols = [f'{f}_left' for f in f_cols]\n",
    "right_cols = [f'{f}_right' for f in f_cols]\n",
    "unknown_cols = [f'{f}_unknown' for f in f_cols]\n",
    "\n",
    "df_ehi['w_sum_f_left'] = df_ehi[left_cols].apply(lambda row: row.sum() / row.count() if not row.isna().all() else np.nan, axis=1)\n",
    "df_ehi['w_sum_f_right'] = df_ehi[right_cols].apply(lambda row: row.sum() / row.count() if not row.isna().all() else np.nan, axis=1)\n",
    "df_ehi['w_sum_f_unknown'] = df_ehi[unknown_cols].apply(lambda row: row.sum() / row.count() if not row.isna().all() else np.nan, axis=1)\n",
    "\n",
    "#IMPORTANT: as of now, if only a few single score has NaN we still calculate the sum of the rest and divide by\n",
    "#the number of answered items. This seems to be the procedure that they followed in their own sums when \n",
    "#inspected visually, but they seem to not do it for all of them. If all scores are NaN, then the sum is set as NaN.\n",
    "\n",
    "\n",
    "#------------- Global Assessment of Functioning (GAF) ---------------------------\n",
    "# OUTPUT dataframe: df_gaf\n",
    "df_gaf = pd.read_excel(dir_input+'gaf_070904.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "\n",
    "#------------- Lab data (labdata) -----------------------------------------------\n",
    "# OUTPUT dataframe: df_labdata\n",
    "df_labdata = pd.read_excel(dir_input+'labdata_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1.*[0-9]$|crfnr)')\n",
    "\n",
    "#Standardising glucose: 1,2,3 units not present. Standardising 4 -> 5.\n",
    "#1 mmol/L = 18.01588 mg/dL, multiply the units of \"4\" by 18.01588.\n",
    "df_labdata.loc[df_labdata['V1l12'] == 4, 'V1l04'] *= 18.01588\n",
    "\n",
    "#Standardising cholesterol: 1,2,3 units not present. Standardising 4 -> 5.\n",
    "#1 mmol/L = 38.67 mg/dL, multiply the units of \"4\" by 38.67.\n",
    "df_labdata.loc[df_labdata['V1l13'] == 4, 'V1l05'] *= 38.67\n",
    "\n",
    "#Standardising LDL: 1,2,3 units not present. Standardising 4 -> 5.\n",
    "#1 mmol/L = 38.67 mg/dL, multiply the units of \"4\" by 38.67.\n",
    "df_labdata.loc[df_labdata['V1l14'] == 4, 'V1l06'] *= 38.67\n",
    "\n",
    "#Standardising HDL: 1,2,3 units not present. Standardising 4 -> 5.\n",
    "#1 mmol/L = 38.67 mg/dL, multiply the units of \"4\" by 38.67.\n",
    "df_labdata.loc[df_labdata['V1l15'] == 4, 'V1l07'] *= 38.67\n",
    "\n",
    "#Standardising triglicerides: 1,2,3 units not present. Standardising 4 -> 5.\n",
    "#1 mmol/L = 88.57 mg/dL, multiply the units of \"4\" by 38.67.\n",
    "df_labdata.loc[df_labdata['V1l16'] == 4, 'V1l09'] *= 38.67\n",
    "\n",
    "#Standardising prolactin: 2,4 and 5 not present. Standardising 1 -> 3 (?).\n",
    "#1 IU/L = 47.76 mcg/L, multiply the units of \"1\" by 47.76.\n",
    "df_labdata.loc[df_labdata['V1l11'] == 1, 'V1l03'] *= 47.76\n",
    "\n",
    "# - Remove irrelevant variable of done/undone and units. \n",
    "df_labdata = df_labdata.drop(columns=['V1l10', 'V1l12', 'V1l13', 'V1l14', 'V1l15', 'V1l16', 'V1l11'])\n",
    "\n",
    "#------------- Manchester Short Assesssment QOF --------------------------------\n",
    "# OUTPUT dataframe: df_mansa\n",
    "df_mansa = pd.read_excel(dir_input+'mansa_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "\n",
    "\n",
    "#------------- Antypsychotic Prescribed and Initial Dose -----------------------\n",
    "# OUTPUT dataframe: df_antipsychotic\n",
    "df_medrec = pd.read_excel(dir_input+'medrec_070727.xlsx', sheet_name = 'Data').filter(regex='^(MR|crfnr|R1_mr02_dat|R1_mr03|R1_mr04_dat)')\n",
    "\n",
    "#Normalise dose given based on the max and min of eah individual drug. \n",
    "for drug_num in [1, 2, 3, 4, 5]:\n",
    "    max_value = df_medrec[df_medrec['MR01'] == drug_num]['R1_mr03'].max()\n",
    "    min_value = df_medrec[df_medrec['MR01'] == drug_num]['R1_mr03'].min()\n",
    "    df_medrec.loc[df_medrec['MR01'] == drug_num, 'NORM_DOSE_MR01'] = (df_medrec[df_medrec['MR01'] == drug_num]['R1_mr03'] - min_value) / (max_value - min_value)\n",
    "\n",
    "# Drop unneded columns for baseline. Dropping antipsychotic given since it is present for all patients in df_antipsychotics below.\n",
    "df_medrec = df_medrec.drop(['R1_mr02_dat', 'R1_mr03', 'R1_mr04_dat', 'MR01'], axis=1)\n",
    "\n",
    "#Load the study arm (type of antipsychotic).\n",
    "df_antipsychotic = pd.read_excel(dir_input+'protocol violators.xlsx', sheet_name = 'Data').filter(regex='^(StudyArm|crfnr)')\n",
    "#Dummy code antipsychotic column \"StudyArm\", add the dummy coded columns and drop the categorical column. \n",
    "df_antipsychotic = pd.concat([df_antipsychotic, pd.get_dummies(df_antipsychotic['StudyArm'], prefix='StudyArm')], axis=1).drop('StudyArm', axis=1)\n",
    "\n",
    "#Merge the drug given and normalised dose. \n",
    "df_antipsychotic = pd.merge(df_antipsychotic, df_medrec, on='crfnr', how='left')\n",
    "\n",
    "\n",
    "#------------- Antipsychotic Prescribed Mapping to Binding Profile -----------------------\n",
    "# OUTPUT dataframe: df_moa_mapping\n",
    "df_medrec = pd.read_excel(dir_input+'medrec_070727.xlsx', sheet_name = 'Data').filter(regex='^(MR|crfnr|R1_mr02_dat|R1_mr03|R1_mr04_dat)')\n",
    "\n",
    "#Normalise dose given based on the max and min of eah individual drug. \n",
    "for drug_num in [1, 2, 3, 4, 5]:\n",
    "    max_value = df_medrec[df_medrec['MR01'] == drug_num]['R1_mr03'].max()\n",
    "    min_value = df_medrec[df_medrec['MR01'] == drug_num]['R1_mr03'].min()\n",
    "    df_medrec.loc[df_medrec['MR01'] == drug_num, 'NORM_DOSE_MR01'] = (df_medrec[df_medrec['MR01'] == drug_num]['R1_mr03'] - min_value) / (max_value - min_value)\n",
    "\n",
    "# Drop unneded columns for baseline. Dropping antipsychotic given since it is present for all patients in df_antipsychotics below.\n",
    "df_medrec = df_medrec.drop(['R1_mr02_dat', 'R1_mr04_dat', 'MR01', 'R1_mr03'], axis=1)\n",
    "\n",
    "#Load the study arm (type of antipsychotic).\n",
    "df_moa_mapping = pd.read_excel(dir_input+'protocol violators.xlsx', sheet_name = 'Data').filter(regex='^(StudyArm|crfnr)')\n",
    "\n",
    "#Merge the drug given and normalised dose. \n",
    "df_moa_mapping = pd.merge(df_moa_mapping, df_medrec, on='crfnr', how='left')\n",
    "\n",
    "\n",
    "list_of_receptors = [\"D1\", \"D2\", \"D3\", \"D4\", \"D5\", \"H1\", \"H2\", \"H3\", \"H4\", \n",
    "                \"5-HT1\", \"5-HT1A\", \"5-HT1B\", \"5-HT1D\", \"5-HT1E\", \"5-HT1F\", \n",
    "                \"5-HT2\", \"5-HT2A\", \"5-HT2B\", \"5-HT2C\", \"5-HT3\", \"5-HT5A\", \n",
    "                \"5-HT6\", \"5-HT7\", \"α1\", \"α1A\", \"α1B\", \"α2\", \"α2A\", \"α2B\", \n",
    "                \"α2C\", \"β1\", \"β2\", \"M\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \n",
    "                \"SERT\", \"NET\", \"DAT\"]\n",
    "\n",
    "#Load the mapping. \n",
    "df_mapping = pd.read_excel('antipsychotic_mapping.xlsx', sheet_name = 'Ki')\n",
    "df_action = pd.read_excel('antipsychotic_mapping.xlsx', sheet_name = 'Action')\n",
    "\n",
    "#Convert to pKi.\n",
    "df_mapping[list_of_receptors] = - np.log10(df_mapping[list_of_receptors]*1e-9)\n",
    "\n",
    "# Multiply the agonist values in df_pKi_mapping by -1.\n",
    "for column in list_of_receptors:\n",
    "    df_mapping.loc[df_action[column] == 1, column] *= -1\n",
    "\n",
    "# Get pKi's for each drug. \n",
    "df_moa_mapping.loc[df_moa_mapping['StudyArm'] == 5, list_of_receptors] = df_mapping.loc[df_mapping['ATC Code'] == 'N05AE04', list_of_receptors].values\n",
    "df_moa_mapping.loc[df_moa_mapping['StudyArm'] == 4, list_of_receptors] = df_mapping.loc[df_mapping['ATC Code'] == 'N05AL05', list_of_receptors].values\n",
    "df_moa_mapping.loc[df_moa_mapping['StudyArm'] == 3, list_of_receptors] = df_mapping.loc[df_mapping['ATC Code'] == 'N05AH04', list_of_receptors].values\n",
    "df_moa_mapping.loc[df_moa_mapping['StudyArm'] == 2, list_of_receptors] = df_mapping.loc[df_mapping['ATC Code'] == 'N05AH03', list_of_receptors].values\n",
    "df_moa_mapping.loc[df_moa_mapping['StudyArm'] == 1, list_of_receptors] = df_mapping.loc[df_mapping['ATC Code'] == 'N05AD01', list_of_receptors].values\n",
    "\n",
    "#Drop unwanted columns\n",
    "df_moa_mapping = df_moa_mapping.drop(['StudyArm', 'NORM_DOSE_MR01'], axis=1)\n",
    "\n",
    "\n",
    "#------------- MINI Psychiatric Diagnoses --------------------------------------\n",
    "# OUTPUT dataframe: df_miniplus\n",
    "df_miniplus = pd.read_excel(dir_input+'miniplus_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "#Remove dates of visit.\n",
    "df_miniplus  = df_miniplus.drop(columns = ['V1miint_dat'])\n",
    "\n",
    "# ------------ Neurocognitive scores -------------------------------------------\n",
    "# OUTPUT dataframe: df_neuro\n",
    "df_neuro = pd.read_excel(dir_input+'neuro_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "#Remove columns with \"info not obtained\"\n",
    "df_neuro  = df_neuro.drop(columns = ['V1NEU001', 'V1NEU007', 'V1NEU010', 'V1NEU047'])\n",
    "#Add trailmaking difference.\n",
    "df_neuro['V1NEU04MINUS01'] = df_neuro['V1NEU04'] - df_neuro['V1NEU01']\n",
    "df_neuro['V1NEU05MINUS02'] = df_neuro['V1NEU05'] - df_neuro['V1NEU02']\n",
    "\n",
    "#Add total across 5 trials.\n",
    "df_neuro['V1NESUMCORR'] = df_neuro[['V1NEU47', 'V1NEU50', 'V1NEU53', 'V1NEU56', 'V1NEU59']].sum(axis=1)\n",
    "df_neuro['V1NESUMINTRU'] = df_neuro[['V1NEU48', 'V1NEU51', 'V1NEU54', 'V1NEU57', 'V1NEU60']].sum(axis=1)\n",
    "df_neuro['V1NESUMREPE'] = df_neuro[['V1NEU49', 'V1NEU52', 'V1NEU55', 'V1NEU58', 'V1NEU61']].sum(axis=1)\n",
    "#If all rows are NaN, then SUM is also NaN. \n",
    "nan_rows = df_neuro[['V1NEU49', 'V1NEU52', 'V1NEU55', 'V1NEU58', 'V1NEU61']].isna().all(axis=1)\n",
    "df_neuro.loc[nan_rows, ['V1NESUMCORR', 'V1NESUMINTRU', 'V1NESUMREPE']] = np.nan\n",
    "\n",
    "# ------------ Other Antipsychotics -------------------------------------------\n",
    "# OUTPUT dataframe: df_otherantip\n",
    "def categorize_drug(atcode):\n",
    "    if type(atcode) == str:\n",
    "        if atcode == 'N05AX08':\n",
    "            return 'OTHERANTIPSY_Risperidone'\n",
    "        elif atcode == 'N05AH02':\n",
    "            return 'OTHERANTIPSY_Clozapine'\n",
    "        elif atcode == 'N05AH03':\n",
    "            return 'OTHERANTIPSY_Olanzapine'\n",
    "        elif atcode == 'N05AH04':\n",
    "            return 'OTHERANTIPSY_Quetiapine'\n",
    "        elif atcode == 'N05AL01':\n",
    "            return 'OTHERANTIPSY_Sulpiride'\n",
    "        elif atcode == 'N05AD01':\n",
    "            return 'OTHERANTIPSY_Haloperidol'\n",
    "        elif atcode == 'N05AF05':\n",
    "            return 'OTHERANTIPSY_Zuclopenthixol'\n",
    "        elif atcode == 'N05AB03':\n",
    "            return 'OTHERANTIPSY_Perphenazine'\n",
    "        elif atcode == 'N05AA01':\n",
    "            return 'OTHERANTIPSY_Chlorpromazine'\n",
    "        elif atcode == 'N05AA02':\n",
    "            return 'OTHERANTIPSY_Levomepromazine'\n",
    "        elif atcode == 'N05AL05':\n",
    "            return 'OTHERANTIPSY_Amisulpride'\n",
    "        elif atcode == 'N05AX07':\n",
    "            return 'OTHERANTIPSY_Prothipendyl'\n",
    "        elif atcode == 'N05AX09':\n",
    "            return 'OTHERANTIPSY_Clotiapine'\n",
    "        elif atcode == 'N05AX12':\n",
    "            return 'OTHERANTIPSY_Aripiprazole'\n",
    "        elif atcode == 'N05AB10':\n",
    "            return 'OTHERANTIPSY_Perazine'\n",
    "        elif atcode == 'N05AF01':\n",
    "            return 'OTHERANTIPSY_Flupentixol'\n",
    "        elif atcode == 'N05AG03':\n",
    "            return 'OTHERANTIPSY_Penfluridol'\n",
    "        elif atcode == 'N05AG02':\n",
    "            return 'OTHERANTIPSY_Pimozide'\n",
    "        elif atcode == 'N05AA03':\n",
    "            return 'OTHERANTIPSY_Promazine'\n",
    "        elif atcode == 'N05AB02':\n",
    "            return 'OTHERANTIPSY_Fluphenazine'\n",
    "        elif atcode == 'N05AH01':\n",
    "            return 'OTHERANTIPSY_Loxapine'\n",
    "        elif atcode == 'N05AE04':\n",
    "            return 'OTHERANTIPSY_Ziprasidone'\n",
    "        elif atcode == 'N05AF03':\n",
    "            return 'OTHERANTIPSY_Chlorprothixene'\n",
    "        elif atcode == 'N05AB06':\n",
    "            return 'OTHERANTIPSY_Trifluoperazine'\n",
    "        elif atcode == 'N05AA06':\n",
    "            return 'OTHERANTIPSY_Cyamemazine'\n",
    "        elif atcode == 'N05AD05':\n",
    "            return 'OTHERANTIPSY_Pipamperone'\n",
    "        elif atcode == 'N05AX11':\n",
    "            return 'OTHERANTIPSY_Zotepine'\n",
    "        elif atcode == 'N05AC02':\n",
    "            return 'OTHERANTIPSY_Thioridazine'\n",
    "        elif atcode == 'N05AB04':\n",
    "            return 'OTHERANTIPSY_Prochlorperazine'\n",
    "        else:\n",
    "            return 'OTHERANTIPSY_OTHER'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_otherantip = pd.read_excel(dir_input+'Other_antipsych_070727.xlsx', sheet_name = 'Data', usecols = ['crfnr','dStart1', 'dEind1', 'atcode1'])\n",
    "#Extract dates of baseline visit. \n",
    "df_v1_date = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(rr001|crfnr)')\n",
    "#Left merge. \n",
    "df_otherantip = pd.merge(df_otherantip, df_v1_date, on='crfnr', how='left')\n",
    "# Create \"other antipsychotic before\" variable, when date of V1 is after start of first other antipsychotic medication.\n",
    "df_otherantip['OTHERANTIPSY_BEFORE'] = (df_otherantip['rr001_dat'] >= df_otherantip['dStart1']).astype(int) \n",
    "# Set OTHERANTIPSY_BEFORE to NaN where R1_pi02_dat is NaN or NaT.\n",
    "df_otherantip.loc[df_otherantip['dStart1'].isna(), 'OTHERANTIPSY_BEFORE'] = np.nan\n",
    "# Story patient ID and CONCOMMED_BEFORE.\n",
    "df_otherantip = df_otherantip[['crfnr','OTHERANTIPSY_BEFORE']]\n",
    "\n",
    "\n",
    "# Define the list of other antipsychotic administrations (it goes up to 25)\n",
    "medications = [str(i) for i in range(1, 30)]\n",
    "\n",
    "#Unique names of other antipsychotics present in the dataset. \n",
    "drug_names = ['OTHERANTIPSY_' + drug for drug in [\n",
    "    'Risperidone', 'Clozapine', 'Olanzapine', 'Quetiapine', 'Sulpiride', 'Haloperidol',\n",
    "    'Zuclopenthixol', 'Perphenazine', 'Chlorpromazine', 'Levomepromazine', 'Amisulpride',\n",
    "    'Prothipendyl', 'Clotiapine', 'Gabapentin', 'Perazine', 'Flupentixol', 'Penfluridol',\n",
    "    'Pimozide', 'Promazine', 'Fluphenazine', 'Loxapine', 'Ziprasidone', 'Chlorprothixene',\n",
    "    'Trifluoperazine', 'Cyamemazine', 'Pipamperone', 'Zotepine', 'Thioridazine', 'Prochlorperazine']]\n",
    "\n",
    "\n",
    "list_of_TYPE = []\n",
    "\n",
    "for medication in medications:\n",
    "    # Read other antipsychotic medication data\n",
    "    cols = ['crfnr','dStart'+medication, 'dEind'+medication, 'atcode'+medication]\n",
    "    df_otherantip_each = pd.read_excel(dir_input+'Other_antipsych_070727.xlsx', sheet_name='Data', usecols = cols)\n",
    "\n",
    "    # Left merge\n",
    "    df_otherantip_each = pd.merge(df_otherantip_each, df_v1_date, on='crfnr', how='left')\n",
    "\n",
    "    # Create \"other antipsychotic medication before\" variable.\n",
    "    df_otherantip_each['OTHERANTIPSY_BEFORE'] = (df_otherantip_each['rr001_dat'] >= df_otherantip_each['dStart'+medication]).astype(int)\n",
    "    \n",
    "    #Mapping of codes to drug name. \n",
    "    for drug in drug_names:\n",
    "        df_otherantip_each[drug] = df_otherantip_each['atcode'+medication].apply(lambda x: 1 if categorize_drug(x) == drug else 0) \n",
    "\n",
    "    #Set dummy codes for types of drugs to 0 if CONCOMMED_BEFORE is 0.\n",
    "    df_otherantip_each.loc[df_otherantip_each['OTHERANTIPSY_BEFORE'] == 0, drug_names] = 0\n",
    "    #Append it to the list. \n",
    "    list_of_TYPE.append(df_otherantip_each[drug_names])\n",
    "\n",
    "    \n",
    "#Record other antipsychotic medication names and overlap them. If one type is prescribed more than once, it is stil set to 1. \n",
    "df_otherantip.loc[:, drug_names] = np.where(np.sum(list_of_TYPE, axis=0)>= 1, 1, 0)\n",
    "# If OTHERANTIPSY_BEFORE is n.nan, then all interventions are nan (since it is not clear if v1 was before or after).\n",
    "df_otherantip.loc[df_otherantip_each['OTHERANTIPSY_BEFORE'].isna(), drug_names] = np.nan\n",
    "\n",
    "\n",
    "#IMPORTANT NEW ADDITION: If there is no record of other antipsychotics, then set OTHERANTIPSY_BEFORE and individual antipsychotic to 0.\n",
    "df_v1 = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(crfnr)')\n",
    "# Get entries of patients not in df_otherantip and set all variables to 0.\n",
    "df_no_otherantip = df_v1[~df_v1['crfnr'].isin(df_otherantip['crfnr'])].copy()\n",
    "df_no_otherantip[['OTHERANTIPSY_BEFORE', drug_names]] = 0\n",
    "# Concatenate entries in the register and not in the register.\n",
    "df_otherantip = pd.concat([df_otherantip, df_no_otherantip]).sort_values(by='crfnr')\n",
    "\n",
    "## NOTE that NaN entries in this dataset involve patients where there is a register of other antipsychotics, but the dates are \n",
    "#not clear and therefore not known whether it was before, during or after baseline.\n",
    "\n",
    "\n",
    "# ------------ PANSS -----------------------------------------------------------\n",
    "# OUTPUT dataframe: df_panss\n",
    "df_panss = pd.read_excel(dir_input+'pansa_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "# List of PANSS score columns.\n",
    "positive_columns = ['V1pa01', 'V1pa02', 'V1pa03', 'V1pa04', 'V1pa05', 'V1pa06', 'V1pa07']\n",
    "negative_columns = ['V1pa08', 'V1pa09', 'V1pa10', 'V1pa11', 'V1pa12', 'V1pa13', 'V1pa14']\n",
    "general_columns = ['V1pa15', 'V1pa16', 'V1pa17', 'V1pa18', 'V1pa19', 'V1pa20', 'V1pa21', 'V1pa22', 'V1pa23', 'V1pa24', 'V1pa25', 'V1pa26', 'V1pa27', 'V1pa28', 'V1pa29', 'V1pa30']\n",
    "\n",
    "# Add new columns for the sum of positive, negative, and general scores\n",
    "df_panss['V1ptotal'] = df_panss[positive_columns].sum(axis=1)\n",
    "df_panss['V1ntotal'] = df_panss[negative_columns].sum(axis=1)\n",
    "df_panss['V1gtotal'] = df_panss[general_columns].sum(axis=1)\n",
    "\n",
    "# Set the sum values to NaN if all individual values are NaN\n",
    "df_panss.loc[df_panss[positive_columns].isna().all(axis=1), 'V1ptotal'] = np.nan\n",
    "df_panss.loc[df_panss[negative_columns].isna().all(axis=1), 'V1ntotal'] = np.nan\n",
    "df_panss.loc[df_panss[general_columns].isna().all(axis=1), 'V1gtotal'] = np.nan\n",
    "\n",
    "#Calculate total score. \n",
    "df_panss['V1total'] = df_panss[['V1ptotal', 'V1ntotal', 'V1gtotal']].sum(axis = 1)\n",
    "\n",
    "# Set the total score to NaN if all individual values are NaN\n",
    "df_panss.loc[df_panss[['V1ptotal', 'V1ntotal', 'V1gtotal']].isna().all(axis=1), 'V1total'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "#------------- DSM Psychosis ---------------------------------------------------\n",
    "# OUTPUT dataframe: df_pd\n",
    "df_pd = pd.read_excel(dir_input+'PD_070727.xlsx', sheet_name = 'Data').filter(regex='^(code|crfnr)')\n",
    "#Convert NaNs to 0 if any diagnose is given (mask = True), otherwise leave as NaN (mask = False).\n",
    "code_list = ['code1', 'code2', 'code3', 'code4', 'code5', 'code6', 'code7']\n",
    "mask = df_pd[code_list].eq(1.0).any(axis=1)\n",
    "df_pd.loc[mask, code_list] = df_pd.loc[mask, code_list].fillna(0)\n",
    "\n",
    "#------------- Protocol Violators ----------------------------------------------\n",
    "# OUTPUT dataframe: df_violators\n",
    "# Load just the protocol violators.\n",
    "df_violators = pd.read_excel(dir_input+'protocol violators.xlsx', sheet_name = 'Data').filter(regex='^(VIOLATOR|crfnr)')\n",
    "\n",
    "#------------- Hospitalisations ------------------------------------------------\n",
    "# OUTPUT dataframe: df_hosp\n",
    "\n",
    "#Extract first hospitalisation of patients. \n",
    "df_hosp = pd.read_excel(dir_input+'psychhospitrec_070727.xlsx', sheet_name = 'Data').filter(regex='^(R1|crfnr)')\n",
    "#Extract dates of baseline visit. \n",
    "df_v1_date = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(rr001|crfnr)')\n",
    "#Left merge. \n",
    "df_hosp  = pd.merge(df_hosp, df_v1_date, on='crfnr', how='left')\n",
    "#Create \"hospitalised before\" variable, when date of V1 is after start of first hospitalisation.\n",
    "df_hosp['HOSP_BEFORE'] = (df_hosp['rr001_dat'] >= df_hosp['R1_phr01_dat']).astype(int)\n",
    "# Remove dates. \n",
    "df_hosp = df_hosp.drop(columns = ['rr001_dat', 'R1_phr01_dat', 'R1_phr02_dat'])\n",
    "## IMPORTANT: I visually inspected that if doing the same analysis for R2 (second hospitalisation), \n",
    "#there are no cases of these happening before V1, so the only hospitalisation that can happen before V1 is R1.\n",
    "\n",
    "#IMPORTANT NEW ADDITION: If there is no record of hospitalisations, then set HOSP_BEFORE to 0.\n",
    "df_v1 = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(crfnr)')\n",
    "# Get entries of patients not in df_hosp and set all variables to 0.\n",
    "df_no_hosp = df_v1[~df_v1['crfnr'].isin(df_hosp['crfnr'])].copy()\n",
    "df_no_hosp['HOSP_BEFORE'] = 0\n",
    "# Concatenate entries in the register and not in the register.\n",
    "df_hosp = pd.concat([df_hosp, df_no_hosp]).sort_values(by='crfnr')\n",
    "\n",
    "## NOTE that NaN entries in this dataset involve patients where there is a register of hospitalisation, but the dates are \n",
    "#not clear and therefore not known whether it was before, during or after baseline.\n",
    "\n",
    "\n",
    "#------------- Interventions ---------------------------------------------------\n",
    "# OUTPUT dataframe: df_interv\n",
    "#Extract first intervention. \n",
    "df_interv = pd.read_excel(dir_input+'psychintervrec_070727.xlsx', sheet_name = 'Data').filter(regex='^(R1|crfnr)(?!.*_text$)')\n",
    "#Extract dates of baseline visit. \n",
    "df_v1_date = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(rr001|crfnr)')\n",
    "#Left merge. \n",
    "df_interv  = pd.merge(df_interv, df_v1_date, on='crfnr', how='left')\n",
    "# Create \"intervention before\" variable, when date of V1 is after start of first intervention.\n",
    "df_interv['INTERV_BEFORE'] = (df_interv['rr001_dat'] >= df_interv['R1_pi02_dat']).astype(int) \n",
    "# Set INTERV_BEFORE to NaN where R1_pi02_dat is NaN or NaT.\n",
    "df_interv.loc[df_interv['R1_pi02_dat'].isna(), 'INTERV_BEFORE'] = np.nan\n",
    "#Drop all columns except for INTERV_BEFORE and ID. \n",
    "df_interv = df_interv[['crfnr','INTERV_BEFORE']]\n",
    "#IMPORTANT: Visually inspected that in all cases where R2 (second intervention) happens before V1, there is a R1 (first intervention) that also happens before V1. \n",
    "## IF WE WANTED NUMBER OF INTERVENTIONS AND TYPES OF INTERVENTIONS BEFORE AND DURING V1:\n",
    "# Define the list of interventions (R1, R2, R3, R4...)\n",
    "interventions = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9']\n",
    "\n",
    "names_of_interventions = ['INTERV_T1', 'INTERV_T2', 'INTERV_T3', 'INTERV_T4', 'INTERV_T5', 'INTERV_T6', 'INTERV_T7', 'INTERV_T8', 'INTERV_T9']\n",
    "\n",
    "list_of_TYPE = []\n",
    "\n",
    "for intervention in interventions:\n",
    "    # Read intervention data\n",
    "    df_interv_each = pd.read_excel(dir_input+'psychintervrec_070727.xlsx', sheet_name='Data').filter(regex=f'^({intervention}|crfnr)(?!.*_text$)')\n",
    "\n",
    "    # Left merge\n",
    "    df_interv_each = pd.merge(df_interv_each, df_v1_date, on='crfnr', how='left')\n",
    "\n",
    "    # Create \"intervention before\" variable\n",
    "    df_interv_each[f'INTERV_BEFORE_{intervention}'] = (df_interv_each['rr001_dat'] >= df_interv_each[f'{intervention}_pi02_dat']).astype(int)\n",
    "\n",
    "    # If the intervention is not before V1, set the intervention type to 0\n",
    "    df_interv_each.loc[df_interv_each[f'INTERV_BEFORE_{intervention}'] != 1, [f'{intervention}_pi01_1', f'{intervention}_pi01_2', f'{intervention}_pi01_3', f'{intervention}_pi01_4', f'{intervention}_pi01_5', f'{intervention}_pi01_6', f'{intervention}_pi01_7', f'{intervention}_pi01_8', f'{intervention}_pi01_9']] = 0\n",
    "    #Append the type of interventions. \n",
    "    list_of_TYPE.append(df_interv_each[[f'{intervention}_pi01_1', f'{intervention}_pi01_2', f'{intervention}_pi01_3', f'{intervention}_pi01_4', f'{intervention}_pi01_5', f'{intervention}_pi01_6', f'{intervention}_pi01_7', f'{intervention}_pi01_8', f'{intervention}_pi01_9']])\n",
    "\n",
    "#Record number of interventions and overlap of recorded interventions in all visits. \n",
    "df_interv[names_of_interventions] = np.where(np.sum(list_of_TYPE, axis=0)>= 1, 1, 0)\n",
    "\n",
    "# If INTERV_BEFORE is n.nan, then all interventions are nan (since it is not clear if v1 was before or after).\n",
    "df_interv.loc[df_interv['INTERV_BEFORE'].isna(), names_of_interventions] = np.nan\n",
    "\n",
    "#IMPORTANT NEW ADDITION: If there is no record of interventions, then set INTERV_BEFORE and individual interventions to 0.\n",
    "df_v1 = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data').filter(regex='^(crfnr)')\n",
    "# Get entries of patients not in df_interv and set all variables to 0.\n",
    "df_no_interv = df_v1[~df_v1['crfnr'].isin(df_interv['crfnr'])].copy()\n",
    "df_no_interv[['INTERV_BEFORE', names_of_interventions]] = 0\n",
    "# Concatenate entries in the register and not in the register.\n",
    "df_interv = pd.concat([df_interv, df_no_interv]).sort_values(by='crfnr')\n",
    "\n",
    "## NOTE that NaN entries in this dataset involve patients where there is a register of intervention, but the dates are \n",
    "#not clear and therefore not known whether it was before, during or after baseline.\n",
    "\n",
    "\n",
    "# ------------ St. Hans Rating Scale -------------------------------------------\n",
    "# OUTPUT dataframe: df_shrs\n",
    "df_shrs_scores = pd.read_excel(dir_input+'shrs_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "#Load the diagnoses from StHans.xlsx\n",
    "df_shrs_diag = pd.read_excel(dir_input+'StHans.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "#Merge it with the raw scores of symptoms. \n",
    "df_shrs = df_shrs_scores.merge(df_shrs_diag, how = 'outer')\n",
    "\n",
    "#------------- Substance Abuse -------------------------------------------------\n",
    "# OUTPUT dataframe: df_sub_abuse\n",
    "# Remove columns with \"specify\" info, columns with \"_s\".\n",
    "df_sub_abuse = pd.read_excel(dir_input+'sub_abuse_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)').filter(regex='[^s]$')\n",
    "# Standardise frequency columns (day/week/month) to day frequency for specific substances.  \n",
    "time_cols = ['V1SAL04', 'V1SAL09', 'V1SAL12', 'V1SAL15', 'V1SAL18', 'V1SAL21', 'V1SAL24']\n",
    "freq_cols = ['V1SAL05', 'V1SAL10', 'V1SAL13', 'V1SAL16', 'V1SAL19', 'V1SAL22', 'V1SAL25']\n",
    "# 1 means day, 2 means week, 3 means month.\n",
    "freq_divisor_mapping = {1.0: 1, 2.0: 7, 3.0: 30}\n",
    "#Normalise \"times\" columns\n",
    "for time_col, freq_col in zip(time_cols, freq_cols):\n",
    "    df_sub_abuse[time_col] = df_sub_abuse[time_col] / df_sub_abuse[freq_col].map(freq_divisor_mapping)\n",
    "    # Drop the original frequency column, not needed any more. \n",
    "    df_sub_abuse.drop(columns=[freq_col], inplace=True)\n",
    "\n",
    "#------------- Side Effects ----------------------------------------------------\n",
    "# OUTPUT dataframe: df_uku\n",
    "df_uku = pd.read_excel(dir_input+'uku_070727.xlsx', sheet_name = 'Data').filter(regex='^(V1|crfnr)')\n",
    "# Remove \"0\" -> Not assessed from some variables. \n",
    "list_of_variables = ['V1UKU08', 'V1UKU09', 'V1UKU10', 'V1UKU11', 'V1UKU12', 'V1UKU13', 'V1UKU21', 'V1UKU22', 'V1UKU23', 'V1UKU24', 'V1UKU25', 'V1UKU26', 'V1UKU27']\n",
    "df_uku[list_of_variables] = df_uku[list_of_variables].replace(0, np.nan)\n",
    "\n",
    "#------------- Visit 1, demographics (and others) and covariants -------------------------------------------\n",
    "# OUTPUT dataframe: df_prognosis, df_covariates, df_demographics\n",
    "df_v1 = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data')\n",
    "df_age = pd.read_excel(dir_input+'age.xlsx', sheet_name = 'Data')\n",
    "\n",
    "#Define the selected columns from the dataset. \n",
    "cols_for_demographics = ['crfnr','a003', 'a015_t', 'a022','a016', 'a017', 'a023', 'a024', 'a025',\n",
    "                        'a027', 'a030','a031', 'a032', 'a033', 'a034', 'a035', 'a036',\n",
    "                        'a037', 'a038', 'a039', 'a040', 'a041', 'a042', 'a043', 'a044', \n",
    "                         'a045', 'a046', 'a047', 'a048', 'a049', 'a050', 'a050_2', 'a052', \n",
    "                         'a053', 'a054', 'a055', 'a056', 'a057', 'a057_2','a059', 'a062', 'a063',\n",
    "                         'e02', 'e03', 'ph01', 'ph03', 'ph02', 'ph05', 'ph04', 'ph06', \n",
    "                        'a060_1', 'a060_2', 'a060_3', 'a060_4', 'a060_5']\n",
    "\n",
    "cols_for_dummy_coding_demographics = ['a015_t', 'a022', 'a031', 'a033', 'a034', 'a035', 'a037', 'a038', 'a039', 'a040', 'a059']\n",
    "\n",
    "\n",
    "#Extract the clinitian prognosis variable. \n",
    "df_prognosis = df_v1[['crfnr', 'p01']]\n",
    "\n",
    "#Extract site as covariate. \n",
    "df_covariates = df_v1[['crfnr', 'sitecode']]\n",
    "#Dummy code the covariates and add them to the dataframe without droping original.\n",
    "df_covariates = pd.concat([df_covariates, pd.get_dummies(df_covariates['sitecode'], columns=['sitecode'], prefix = 'sitecode').astype(int)], axis = 1)\n",
    "\n",
    "\n",
    "#Extraction of selected demographic variables; raw. \n",
    "df_demographics = df_v1[cols_for_demographics].copy()\n",
    "\n",
    "#Case \"a031\" has nan values where it should be 8 -> 'unemployed' because they have answered \"a030\" of current occupation as \"no\". \n",
    "#So changing nan for 8 when the other column is 0. \n",
    "df_demographics.loc[df_demographics['a030'] == 0, 'a031'] = 8\n",
    "#Same applies to columns a032 and a033. \n",
    "df_demographics.loc[df_demographics['a032'] == 0, 'a033'] = 8\n",
    "\n",
    "# Normalisation of income amount per landcode (country) based on the max and min of eah individual country. \n",
    "df_demographics['a062'] = pd.to_numeric(df_demographics['a062'], errors = 'coerce')\n",
    "df_demographics['a062_norm'] = df_demographics['a062']\n",
    "for landcode_num in df_v1['landcode'].unique():\n",
    "    max_value = df_demographics[df_v1['landcode'] == landcode_num]['a062'].max()\n",
    "    min_value = df_demographics[df_v1['landcode'] == landcode_num]['a062'].min()\n",
    "    df_demographics.loc[df_v1['landcode'] == landcode_num, 'a062_norm'] = (df_demographics[df_v1['landcode'] == landcode_num]['a062'] - min_value) / (max_value - min_value)\n",
    "\n",
    "#Drop the original variable.\n",
    "df_demographics['a062'] = df_demographics['a062_norm']\n",
    "#Retain the location of the \"a062\" variable. \n",
    "df_demographics.drop('a062_norm', axis = 1, inplace = True)\n",
    "df_demographics.rename(columns={'a062': 'a062_norm'}, inplace=True)\n",
    "\n",
    "#Calculate BMI from weight and height (metric system).\n",
    "df_demographics['phbmi'] = df_demographics['ph02'] / (df_demographics['ph01']**2)\n",
    "\n",
    "# Dummy coding of selected variables. \n",
    "for col in cols_for_dummy_coding_demographics:\n",
    "    df_dummies = pd.get_dummies(df_demographics[col], prefix=col).astype(int)\n",
    "    #If the rows are all 0's (meaning the original var was NaN) set the dummy variables to NaN.\n",
    "    mask_all_zeros = (df_dummies.sum(axis=1) == 0)\n",
    "    df_dummies[df_dummies.sum(axis=1) == 0] = np.nan\n",
    "    #Add to dataframe and remove original.\n",
    "    # Insert dummy columns at the same location as the original variable.\n",
    "    col_index = df_demographics.columns.get_loc(col)\n",
    "    df_demographics = pd.concat([df_demographics.iloc[:, :col_index], df_dummies, df_demographics.iloc[:, col_index+1:]], axis=1)\n",
    "\n",
    "# Merge age to df_demographics. \n",
    "df_demographics = df_demographics.merge(df_age, how = 'left', on = 'crfnr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834c35fb-d868-44e0-bc5d-67ad3c372e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#Storing Data and Merging\n",
    "################################################################################\n",
    "# Sergio Mena Ortega, 2023\n",
    "\n",
    "\n",
    "#Writing individual datasets into excel.\n",
    "## IMPORTANT: violators not removed, bear in mind they will have to be removed if using these.\n",
    "df_cam.to_excel(dir_output+'Individual Datasets/can.xlsx', index = None) #\n",
    "df_cdss.to_excel(dir_output+'Individual Datasets/cdss.xlsx', index = None) #\n",
    "df_cgi.to_excel(dir_output+'Individual Datasets/cgi.xlsx', index = None) #\n",
    "df_concommed.to_excel(dir_output+'Individual Datasets/concommed.xlsx', index = None) #\n",
    "df_ehi.to_excel(dir_output+'Individual Datasets/ehi.xlsx', index = None) #\n",
    "df_gaf.to_excel(dir_output+'Individual Datasets/gaf.xlsx', index = None) #\n",
    "df_labdata.to_excel(dir_output+'Individual Datasets/labdata.xlsx', index = None) #\n",
    "df_mansa.to_excel(dir_output+'Individual Datasets/mansa.xlsx', index = None) #\n",
    "df_antipsychotic.to_excel(dir_output+'Individual Datasets/antipsychotics.xlsx', index = None) #\n",
    "df_moa_mapping.to_excel(dir_output+'Individual Datasets/moa_mapping.xlsx', index = None)\n",
    "df_miniplus.to_excel(dir_output+'Individual Datasets/miniplus.xlsx', index = None) #\n",
    "df_neuro.to_excel(dir_output+'Individual Datasets/neuro.xlsx', index = None) #\n",
    "df_otherantip.to_excel(dir_output+'Individual Datasets/otherantipsychotic.xlsx', index = None) #\n",
    "df_panss.to_excel(dir_output+'Individual Datasets/panss.xlsx', index = None) #\n",
    "df_violators.to_excel(dir_output+'Individual Datasets/protocolviolators.xlsx', index = None)\n",
    "df_hosp.to_excel(dir_output+'Individual Datasets/hospitalisations.xlsx', index = None) #\n",
    "df_interv.to_excel(dir_output+'Individual Datasets/interventions.xlsx', index = None) #\n",
    "df_shrs.to_excel(dir_output+'Individual Datasets/shrs.xlsx', index = None) #\n",
    "df_sub_abuse.to_excel(dir_output+'Individual Datasets/sub_abuse.xlsx', index = None) #\n",
    "df_uku.to_excel(dir_output+'Individual Datasets/uku.xlsx', index = None) #\n",
    "df_prognosis.to_excel(dir_output+'Individual Datasets/prognosis.xlsx', index = None) #\n",
    "df_covariates.to_excel(dir_output+'Individual Datasets/covariates.xlsx', index = None) #\n",
    "df_demographics.to_excel(dir_output+'Individual Datasets/demographics.xlsx', index = None) #\n",
    "\n",
    "#Merging data into 4 categories: clinical+sociodemographic, cognitive, QoL and lab data (df_cdss, df_concommed, df_ehi, df_gaf, df_antipsychotic,\n",
    "#df_miniplus, df_otherantip, df_panss, df_hosp, df_interv, df_shrs, df_sub_abuse, df_uku, df_demographics).\n",
    "\n",
    "#Get violators IDs.\n",
    "violators = df_violators.loc[df_violators['VIOLATOR'] == 1, 'crfnr']\n",
    "\n",
    "#Get the original ids. \n",
    "df_id = pd.read_excel(dir_input+'v1_070727.xlsx', sheet_name = 'Data', usecols = ['crfnr'])\n",
    "\n",
    "#Clinical and sociodemographic data category.\n",
    "df_clin_and_socio = df_id.copy()\n",
    "list_of_dataframes = [df_cdss, df_cgi, df_concommed, df_ehi, df_gaf, df_antipsychotic, df_miniplus, df_otherantip, df_panss, df_hosp, df_interv, df_shrs, df_sub_abuse, df_uku, df_demographics, df_pd] \n",
    "for data in list_of_dataframes:\n",
    "    df_clin_and_socio = pd.merge(df_clin_and_socio, data, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_clin_and_socio = df_clin_and_socio[~df_clin_and_socio['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_clin_and_socio.to_excel(dir_output+'Merged Datasets/clinical_and_sociodemographic.xlsx', index = False)\n",
    "\n",
    "#MOA mapping category (df_moa_mapping). \n",
    "df_moa_mapping_data = pd.merge(df_id, df_moa_mapping, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_moa_mapping_data = df_moa_mapping_data[~df_moa_mapping_data['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_moa_mapping_data.to_excel(dir_output+'Merged Datasets/moa_mapping.xlsx', index = False)\n",
    "\n",
    "#Blood data category (df_labdata). \n",
    "df_blood_data = pd.merge(df_id, df_labdata, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_blood_data = df_blood_data[~df_blood_data['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_blood_data.to_excel(dir_output+'Merged Datasets/lab_data.xlsx', index = False)\n",
    "\n",
    "\n",
    "#Cognitive category (df_neuro). \n",
    "df_cognitive = pd.merge(df_id, df_neuro, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_cognitive = df_cognitive[~df_cognitive['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_cognitive.to_excel(dir_output+'Merged Datasets/cognitive.xlsx', index = False)\n",
    "\n",
    "\n",
    "##Quality of life category (df_mansa, df_cam). \n",
    "df_quality_of_life = pd.merge(df_id, df_cam, on='crfnr', how='left')\n",
    "df_quality_of_life = pd.merge(df_quality_of_life, df_mansa, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_quality_of_life = df_quality_of_life[~df_quality_of_life['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_quality_of_life.to_excel(dir_output+'Merged Datasets/quality_of_life.xlsx', index = False)\n",
    "\n",
    "\n",
    "##Clinicians prognosis (df_prognosis). \n",
    "df_prognosis = pd.merge(df_id, df_prognosis, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_prognosis = df_prognosis[~df_prognosis['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_prognosis.to_excel(dir_output+'Merged Datasets/prognosis.xlsx', index = False)\n",
    "\n",
    "##Covariates (df_covariates).\n",
    "df_covariates = pd.merge(df_id, df_covariates, on='crfnr', how='left')\n",
    "#Remove protocol violator entries. \n",
    "df_covariates = df_covariates[~df_covariates['crfnr'].isin(violators)]\n",
    "#Save to excel.    \n",
    "df_covariates.to_excel(dir_output+'Merged Datasets/covariates.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
